defaults:
  - model:
      - qwen25-7b

training:
  global_batch_size: 12
  seq_length: 4096
  tokenizer_type: PretrainedFromHF
  tokenizer_name_or_path: ./models/Qwen2.5-Math-7B
  train_iters: 100
  distributed_backend: nccl
  no_shared_storage: true
  save_interval: 10
  no_load_optim: true
  no_load_rng: true
  bf16: true
  is_instruction_dataset: true
  variable_seq_lengths: true
  no_shuffle: false
  stage: ray_grpo
  sequence_parallel: true
  dataset_with_labels: true

actor_rollout_ref:
  actor_rollout:
    model: qwen25-7b
    do_sample: true
    micro_batch_size: 2
    ppo_mini_batch_size: 12
    num_samples_per_step: 2
    temperature: 1.0
    max_prompt_length: 1536
    max-tokens-to-oom: 1000000
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.001
    shuffle_minibatch: false
    use_kv_cache: true
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 4
    lr: 1e-6
    lr_decay_style: constant
    min_lr: 0.0
    weight_decay: 0.0
    lr_warmup_fraction: 0.0
    clip_grad: 10000.0
    initial_loss_scale: 4096
    finetune: true
    load: ./ckpt
    save: ./ckpt
    num_gpus_for_train: 8
    num_gpus_for_infer: 6
    data_path: ./dataset/pe-nlp/data
    split: 100,0,0
    n_samples_per_prompt: 4

  ref:
    model: qwen25-7b
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 1
    micro_batch_size: 4
    load: ./ckpt

reward:
  model: qwen25-7b
  verifier: true
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 1
  micro_batch_size: 4
  load: ./ckpt

algorithm:
  gamma: 1.0
  lam: 0.95
  adv_estimator: group_norm
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.05
  missing_eos_penalty: 0.0
  verifier_function: ["acc", "format"]
  verifier_weight: [0.5, 0.5]

resource_pool:
  actor_rollout: [6, 8]
  ref: [2]
  reward: []