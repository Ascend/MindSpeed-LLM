defaults:
  - model:
      - llama3-8b

training:
  global_batch_size: 2
  seq_length: 512
  tokenizer_type: PretrainedFromHF
  tokenizer_name_or_path: ./models/llama-3-8b/
  train_iters: 1000
  distributed_backend: nccl
  no_shared_storage: true
  save_interval: 10000
  no_load_optim: true
  no_load_rng: true
  bf16: true
  is_instruction_dataset: true
  variable_seq_lengths: true
  no_shuffle: true
  stage: ray_ppo
  sequence_parallel: False

actor_rollout_ref:
  actor_rollout:
    model: llama3-8b
    do_sample: false
    micro_batch_size: 1
    ppo_mini_batch_size: 1
    num_samples_per_step: 1
    max_prompt_length: 256
    ppo_epochs: 1
    clip_ratio: 0.2
    entropy_coeff: 0.001
    shuffle_minibatch: false
    use_kv_cache: true
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 2
    lr: 1e-7
    lr_decay_style: constant
    min_lr: 0.0
    weight_decay: 0.0
    lr_warmup_fraction: 0.0
    clip_grad: 1.0
    adam_beta1: 0.9
    adam_beta2: 0.999
    initial_loss_scale: 4096
    finetune: true
    load: ./ckpt
    save: ./ckpt
    num_gpus_for_train: 4
    num_gpus_for_infer: 4
    pad_to_multiple_of: 1
    data_path: ./dataset/descriptiveness/descriptiveness
    split: 100,0,0

  ref:
    model: llama3-8b
    tensor_model_parallel_size: 2
    pipeline_model_parallel_size: 1
    micro_batch_size: 1
    load: ./ckpt

critic:
  model: llama3-8b
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 2
  use_mcore_models: True
  micro_batch_size: 1
  lr: 1e-7
  lr_decay_style: constant
  min_lr: 0.0
  weight_decay: 0.0
  lr_warmup_fraction: 0.0
  use_distributed_optimizer: true
  clip_grad: 1.0
  adam_beta1: 0.9
  adam_beta2: 0.999
  initial_loss_scale: 1
  no_load_optim: True
  no_load_rng: True
  load: ./ckpt
  save: ./ckpt
  cliprange_value: 0.2
  critic_mini_batch_size: 1
  critic_update_epochs: 1

reward:
  model: llama3-8b
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 1
  micro_batch_size: 1
  load: ./ckpt

algorithm:
  gamma: 1.0
  lam: 0.95
  adv_estimator: gae
  kl_penalty: kl
  kl_ctrl:
    type: fixed
    kl_coef: 0.05
  missing_eos_penalty: 0.0

resource_pool:
  actor_rollout: [8]
  ref: [2]
  critic: [4]
  reward: [2]