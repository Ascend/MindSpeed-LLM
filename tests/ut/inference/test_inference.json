{
    "test_llama2_mcore_prompt_greedy_search": [
        {
            "param": {
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "use-mcore-models": null,
                "use-kv-cache": null,
                "use-flash-attn": null,
                "use-fused-swiglu": null,
                "use-fused-rmsnorm": null,
                "use-fused-rotary-pos-emb": null,
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "position-embedding-type": "rope", 
                "seq-length": 4096, 
                "max-new-tokens": 30,
                "micro-batch-size": 1,
                "global-batch-size": 1,
                "num-attention-heads": 32,
                "max-position-embeddings": 4096,
                "make-vocab-size-divisible-by": 1,
                "swiglu": null,
                "normalization": "RMSNorm", 
                "load":"/data/ci/ckpt",
                "tokenizer-type": "PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/llama-2-7b-hf",
                "tokenizer-model": "/data/llama-2-7b-hf/tokenizer.model", 
                "disable-bias-linear": null,
                "attention-softmax-in-fp32": null, 
                "untie-embeddings-and-output-weights": null, 
                "no-masked-softmax-fusion": null, 
                "no-load-optim": null, 
                "no-load-rng": null, 
                "fp16": null,
                "task":"greedy",
                "use-deter-comp": null,
                "prompt-type": "llama2"
            }
        }
    ],
    
    "test_llama2_legacy_prompt_greedy_search": [
        {
            "param": {
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "use-kv-cache": null,
                "use-flash-attn": null,
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "num-attention-heads": 32,
                "seq-length": 4096, 
                "max-position-embeddings": 4096,
                "make-vocab-size-divisible-by": 1,
                "normalization": "RMSNorm", 
                "position-embedding-type": "rope", 
                "load":"/data/llama2-7B-tp8-pp1",
                "tokenizer-type": "PretrainedFromHF",  
                "tokenizer-name-or-path":"/data/llama-2-7b-hf",
                "tokenizer-model": "/data/llama-2-7b-hf/tokenizer.model", 
                "disable-bias-linear": null,
                "use-fused-rmsnorm": null, 
                "swiglu": null,
                "attention-softmax-in-fp32": null, 
                "untie-embeddings-and-output-weights": null, 
                "no-masked-softmax-fusion": null, 
                "no-load-optim": null, 
                "no-load-rng": null, 
                "fp16": null,
                "task":"greedy",
                "max-new-tokens": 30,
                "micro-batch-size": 4,
                "global-batch-size": 16,
                "use-deter-comp": null,
                "prompt-type": "llama2"
            }
        }
    ],

    "test_llama2_lora_prompt_legacy_greedy_search": [
        {
            "param": {
                "tensor-model-parallel-size": 8,
                "pipeline-model-parallel-size": 1,
                "num-layers": 32,
                "hidden-size": 4096,
                "ffn-hidden-size": 11008,
                "position-embedding-type": "rope",
                "seq-length": 4096,
                "max-new-tokens": 30,
                "micro-batch-size": 4,
                "global-batch-size": 16,
                "num-attention-heads": 32,
                "max-position-embeddings": 4096 ,
                "swiglu": null,
                "load": "/data/llama2-7B-tp8-pp1",
                "tokenizer-type": "PretrainedFromHF",
                "tokenizer-name-or-path": "/data/llama-2-7b-hf",
                "tokenizer-model": "/data/llama-2-7b-hf/tokenizer.model",
                "tokenizer-not-use-fast": null,
                "fp16": null,
                "normalization": "RMSNorm" ,
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "attention-softmax-in-fp32": null,
                "no-load-optim": null,
                "no-load-rng": null,
                "no-masked-softmax-fusion": null,
                "no-gradient-accumulation-fusion": null,
                "exit-on-missing-checkpoint": null,
                "lora-load": "/data/llama-2-7b-lora-tp8-pp1",
                "lora-r": 16,
                "lora-alpha": 32,
                "task": "greedy",
                "use-deter-comp": null,
                "lora-target-modules": ["query_key_value", "dense", "dense_h_to_4h","dense_4h_to_h"],
                "make-vocab-size-divisible-by": 1,
                "prompt-type": "llama2"
            }
        }
    ],

    "test_deepseek2_mcore_greedy_search": [
        {
            "param": {
                "use-mcore-models": null,
                "spec": ["mindspeed_llm.tasks.models.spec.deepseek_spec", "layer_spec"],
                "tensor-model-parallel-size": 1,
                "pipeline-model-parallel-size": 1,
                "expert-model-parallel-size": 8,
                "use-flash-attn": null,
                "num-layers": 4,
                "hidden-size": 5120,
                "ffn-hidden-size": 12288,
                "seq-length": 8192,
                "max-new-tokens": 30,
                "micro-batch-size": 1,
                "global-batch-size": 16,
                "num-attention-heads": 128,
                "max-position-embeddings": 163840,
                "position-embedding-type": "rope",
                "swiglu": null,
                "tokenizer-type": "PretrainedFromHF",
                "tokenizer-name-or-path": "/data/gemm/",
                "bf16": null,
                "normalization": "RMSNorm",
                "untie-embeddings-and-output-weights": null,
                "disable-bias-linear": null,
                "attention-softmax-in-fp32": null,
                "no-load-optim": null,
                "no-load-rng": null,
                "no-masked-softmax-fusion": null,
                "no-gradient-accumulation-fusion": null,
                "task": "greedy",
                "use-deter-comp": null,
                "make-vocab-size-divisible-by": 1,
                "shape-order": "BNSD",
                "output-layer-slice-num": 10,
                "use-fused-swiglu": null,
                "use-fused-rmsnorm": null,
                "use-fused-rotary-pos-emb": null,
                "use-rotary-position-embeddings": null,
                "vocab-size": 102400,
                "padded-vocab-size": 102400,
                "rotary-base": 10000,
                "norm-epsilon": 1e-6,
                "multi-head-latent-attention": null,
                "qk-rope-head-dim": 64,
                "qk-nope-head-dim": 128,
                "q-lora-rank": 1536,
                "kv-lora-rank": 512,
                "v-head-dim": 128,
                "qk-layernorm": null,
                "moe-grouped-gemm": null,
                "moe-permutation-async-comm": null,
                "moe-token-dispatcher-type": "alltoall",
                "use-fused-moe-token-permute-and-unpermute": null,
                "first-k-dense-replace": 1,
                "moe-layer-freq": 1,
                "n-shared-experts": 2,
                "num-experts": 160,
                "moe-router-topk": 6,
                "moe-intermediate-size": 1536,
                "moe-router-load-balancing-type": "group_limited_greedy",
                "topk-group": 3,
                "moe-aux-loss-coeff": 0.003,
                "moe-device-level-aux-loss-coeff": 0.05,
                "moe-comm-aux-loss-coeff": 0.02,
                "routed-scaling-factor": 16.0,
                "seq-aux": null,
                "rope-scaling-beta-fast": 32,
                "rope-scaling-beta-slow": 1,
                "rope-scaling-factor": 40,
                "rope-scaling-mscale": 1.0,
                "rope-scaling-mscale-all-dim": 1.0,
                "rope-scaling-original-max-position-embeddings": 4096,
                "rope-scaling-type": "yarn"
            }
        }
    ]
}