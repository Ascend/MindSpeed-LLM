model:
  model_id: gpt_oss
  model_name_or_path: /home/data/gpt-oss-20b-hf/
  trust_remote_code: False
  train_from_scratch: False
  tokenizer_name_or_path: /home/data/gpt-oss-20b-hf/

data:
  dataset: alpaca_full
  template: gpt
  cutoff_len: 131072
  max_samples: 100000
  overwrite_cache: True
  preprocessing_num_workers: 1
  packing: True

parallel:
  fsdp_size: 64
  fsdp_modules:
    - model.layers.{*}
    - model.embed_tokens
    - lm_head
  tp_size: 1
  ep_size: 1
  ep_modules:
    - model.layers.{*}.mlp.experts
  ep_fsdp_size: 1
  ep_fsdp_modules:
    - model.layers.{*}.mlp.experts
  ep_dispatcher: eager
  recompute: True
  recompute_modules:
    - model.layers.{*}
  cp_size: 32
  cp_type: ulysses

training:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  dataloader_num_workers : 1
  disable_shuffling: 1
  seed: 42
  dataloader_drop_last: True
  output_dir: ./output
  optimizer: adamw
  lr: 1e-05
  weight_decay: 0.01
  adam_beta1: 0.9
  adam_beta2: 0.95
  adam_epsilon: 1e-08
  max_grad_norm: 1.0
  lr_scheduler_type: cosine
  warmup_ratio: 0.0
  min_lr: 1e-06
  num_train_epochs: 1000.0
  max_steps: -1
  save_steps: 10000
  logging_steps: 1
  use_fused_rmsnorm: True
  use_fused_rotary_pos_emb: True
  use_flash_attn: True