# 全量参数说明

## 模型参数（ModelArguments）

包含模型与分词器加载、初始化相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>model_name_or_path</td>
      <td>str</td>
      <td>无（必填）</td>
      <td>模型的本地路径，必填项，未指定会抛出异常。</td>
    </tr>
    <tr>
      <td>model_id</td>
      <td>Optional[Literal[&quot;gpt_oss&quot;, &quot;qwen3&quot;, &quot;qwen3_moe&quot;]]</td>
      <td>None</td>
      <td>模型类型标识，未配置时执行原生transformer模型前向，配置时执行仓库自定义模型前向。新增模型类型需在<code>mindspeed_llm/fsdp2/models/model_registry.py</code>的<code>ModelRegistry</code>类中注册。</td>
    </tr>
    <tr>
      <td>init_model_with_meta_device</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使用meta device初始化模型，启用后可节省模型初始化阶段的显存占用。</td>
    </tr>
    <tr>
      <td>trust_remote_code</td>
      <td>bool</td>
      <td>False</td>
      <td>是否允许加载Hugging Face上自定义建模文件中的模型，用于适配自定义模型架构。</td>
    </tr>
    <tr>
      <td>train_from_scratch</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使用随机权重从头开始训练模型，不加载模型权重。</td>
    </tr>
    <tr>
      <td>tokenizer_name_or_path</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>Tokenizer的路径或名称，与<code>model_name_or_path</code>路径不一致时需要指定。</td>
    </tr>
    <tr>
      <td>cache_dir</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>模型缓存目录，用于存储从Hugging Face、ModelScope下载的模型。</td>
    </tr>
    <tr>
      <td>use_fast_tokenizer</td>
      <td>bool</td>
      <td>True</td>
      <td>是否使用tokenizers库实现的快速Tokenizer，相比传统Tokenizer分词速度更快。</td>
    </tr>
    <tr>
      <td>resize_vocab</td>
      <td>bool</td>
      <td>False</td>
      <td>是否调整Tokenizer大小及模型对应output_layer/lm_head的维度，用于适配新增token后的词表扩展场景。</td>
    </tr>
    <tr>
      <td>split_special_tokens</td>
      <td>bool</td>
      <td>False</td>
      <td>分词过程中是否拆分特殊token，默认不拆分，保持特殊token的完整性。</td>
    </tr>
    <tr>
      <td>add_tokens</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>需添加到Tokenizer的非特殊token，多个token用逗号分隔。</td>
    </tr>
    <tr>
      <td>add_special_tokens</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>需添加到Tokenizer的特殊token，多个token用逗号分隔。</td>
    </tr>
    <tr>
      <td>new_special_tokens_config</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>特殊token语义初始化的YAML配置文件路径，格式为<code>{&#39;&lt;token&gt;&#39;: &#39;description text&#39;}</code>，优先级高于<code>add_special_tokens</code>。</td>
    </tr>
    <tr>
      <td>init_special_tokens</td>
      <td>Literal[&quot;noise_init&quot;, &quot;desc_init&quot;, &quot;desc_init_w_noise&quot;]</td>
      <td>noise_init</td>
      <td>新增特殊token的初始化方式：1. noise_init（默认）：基于均值的随机噪声初始化；2. desc_init：基于<code>new_special_tokens_config</code>中描述的语义初始化（需配置该参数）；3. desc_init_w_noise：语义初始化+随机噪声。</td>
    </tr>
    <tr>
      <td>model_revision</td>
      <td>str</td>
      <td>main</td>
      <td>指定使用的模型版本，可填写分支名、标签名或commit id。</td>
    </tr>
    <tr>
      <td>hf_hub_token</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>Hugging Face Hub的认证令牌，适用于从Hugging Face下载/上传模型的场景。</td>
    </tr>
    <tr>
      <td>ms_hub_token</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>ModelScope Hub的认证令牌，适用于从ModelScope下载/上传模型的场景。</td>
    </tr>
    <tr>
      <td>om_hub_token</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>Modelers Hub的认证令牌，适用于从Modelers下载/上传模型的场景。</td>
    </tr>
    <tr>
      <td>quant_recipe</td>
      <td>Literal[&quot;mxfp8&quot;]</td>
      <td>None</td>
      <td>量化策略。</td>
    </tr>
    <tr>
      <td>quant_format</td>
      <td>Literal[&quot;E4M3&quot;, &quot;E5M2&quot;, &quot;HIF8&quot;]</td>
      <td>None</td>
      <td>量化数据格式。</td>
    </tr>
    <tr>
      <td>quant_block_size</td>
      <td>int</td>
      <td>32</td>
      <td>量化的块大小。</td>
    </tr>
    <tr>
      <td>quant_apply_modules</td>
      <td>List[str]</td>
      <td>['model.layers.{*}']</td>
      <td>应用量化的层或模块。</td>
    </tr>
    <tr>
      <td>quant_ignored_modules</td>
      <td>List[str]</td>
      <td>['lm_head']</td>
      <td>不应用量化的子模块列表。</td>
    </tr>
    <tr>
      <td>converters</td>
      <td>List[str]</td>
      <td>["quantize.linear.mx"]</td>
      <td>使用的量化转换器列表。</td>
    </tr>
  </tbody>
</table>

## 数据参数（DataArguments）

包含数据集加载、预处理及数据格式相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>template</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>构建prompt的模板名称。</td>
    </tr>
    <tr>
      <td>dataset</td>
      <td>Optional[Union[Dict[str, Any], str]]</td>
      <td>None</td>
      <td>训练数据集：微调场景数据集配置支持内联配置和通过<code>dataset_info.json</code>注册两种方式，预训练场景数据集配置直接填写原始数据集路径。具体配置示例参考<a href="./fsdp2_quickstart.md">FSDP2后端训练使用指南</a>，此处提供简单配置示例：<pre style="text-align: left;"><code>dataset:<br>&nbsp;&nbsp;file_name: "./my_data.json"   # 数据文件路径<br>&nbsp;&nbsp;formatting: "alpaca"          # 数据格式</code></pre></td>
    </tr>
    <tr>
      <td>eval_dataset</td>
      <td>Optional[Union[Dict[str, Any], str]]</td>
      <td>None</td>
      <td>评估数据集：填写方式与<code>dataset</code>一致。与<code>val_size</code>互斥，不可同时设置。</td>
    </tr>
    <tr>
      <td>dataset_dir</td>
      <td>str</td>
      <td>./configs/fsdp2/data</td>
      <td>数据集配置文件所在目录。</td>
    </tr>
    <tr>
      <td>cutoff_len</td>
      <td>int</td>
      <td>2048</td>
      <td>分词后输入序列的截断长度，超过该长度的序列会被截断。</td>
    </tr>
    <tr>
      <td>train_on_prompt</td>
      <td>bool</td>
      <td>False</td>
      <td>是否取消prompt部分的掩码，与<code>mask_history</code>不可同时为True。</td>
    </tr>
    <tr>
      <td>mask_history</td>
      <td>bool</td>
      <td>False</td>
      <td>是否掩码对话历史，仅在最后一轮回复上训练；True时对话历史部分不计算损失，与<code>train_on_prompt</code>不可同时为True。</td>
    </tr>
    <tr>
      <td>streaming</td>
      <td>bool</td>
      <td>False</td>
      <td>是否启用数据集流式加载，与<code>max_samples</code>互斥，且<code>val_size</code>需为整数。</td>
    </tr>
    <tr>
      <td>buffer_size</td>
      <td>int</td>
      <td>16384</td>
      <td>流式加载时的随机采样缓冲区大小。</td>
    </tr>
    <tr>
      <td>mix_strategy</td>
      <td>Literal[&quot;concat&quot;, &quot;interleave_under&quot;, &quot;interleave_over&quot;]</td>
      <td>concat</td>
      <td>多数据集混合策略：拼接/欠采样/过采样。</td>
    </tr>
    <tr>
      <td>interleave_probs</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>多数据集交叉采样概率，使用逗号分隔，仅在<code>mix_strategy</code>为interleave_under/interleave_over时生效。</td>
    </tr>
    <tr>
      <td>overwrite_cache</td>
      <td>bool</td>
      <td>False</td>
      <td>是否覆盖已缓存的预处理后数据集。</td>
    </tr>
    <tr>
      <td>preprocessing_batch_size</td>
      <td>int</td>
      <td>1000</td>
      <td>数据预处理时每组的样本数量。</td>
    </tr>
    <tr>
      <td>preprocessing_num_workers</td>
      <td>Optional[int]</td>
      <td>None</td>
      <td>数据预处理的进程数。</td>
    </tr>
    <tr>
      <td>max_samples</td>
      <td>Optional[int]</td>
      <td>None</td>
      <td>调试用，用于截断每个数据集的样本数量，与<code>streaming</code>互斥。</td>
    </tr>
    <tr>
      <td>eval_num_beams</td>
      <td>Optional[int]</td>
      <td>None</td>
      <td>评估时<code>model.generate</code>使用的beam数量。</td>
    </tr>
    <tr>
      <td>ignore_pad_token_for_loss</td>
      <td>bool</td>
      <td>True</td>
      <td>损失计算时是否忽略填充标签对应的token。</td>
    </tr>
    <tr>
      <td>val_size</td>
      <td>float</td>
      <td>0.0</td>
      <td>验证集大小，整数或0~1的浮点数。需同时指定<code>dataset</code>，与<code>eval_dataset</code>互斥。</td>
    </tr>
    <tr>
      <td>eval_on_each_dataset</td>
      <td>bool</td>
      <td>False</td>
      <td>是否分别在每个数据集上进行评估。</td>
    </tr>
    <tr>
      <td>packing</td>
      <td>bool</td>
      <td>False</td>
      <td>是否启用序列打包，将多个短序列打包为一个长序列。</td>
    </tr>
    <tr>
      <td>neat_packing</td>
      <td>bool</td>
      <td>False</td>
      <td>是否启用无交叉注意力的序列打包，启用后自动设置<code>packing=True</code>。</td>
    </tr>
    <tr>
      <td>tool_format</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>构建函数调用示例使用的工具格式，用于适配工具调用类任务，统一函数调用的格式规范。</td>
    </tr>
    <tr>
      <td>default_system</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>覆盖模板中的默认系统提示词，用于自定义系统提示。</td>
    </tr>
    <tr>
      <td>enable_thinking</td>
      <td>Optional[bool]</td>
      <td>True</td>
      <td>是否为推理模型启用思考模式，启用后模型会生成中间思考过程。True表示启用，False表示不启用，None表示不删除原始数据中的<code>cot</code>标签，适用于原始数据为混合类型的情况。</td>
    </tr>
    <tr>
      <td>tokenized_path</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>保存或加载已分词数据集的路径。路径不存在时保存分词后的数据集；路径存在时加载已分词的数据集。</td>
    </tr>
    <tr>
      <td>data_shared_file_system</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使用共享文件系统存储数据集，适用于分布式训练场景。</td>
    </tr>
    <tr>
      <td>data_manager_type</td>
      <td>Literal[&quot;lf&quot;, &quot;mg&quot;]</td>
      <td>lf</td>
      <td>数据管理器类型，用于构建不同的数据集管理器，适配不同数据集加载和处理逻辑。lf表示微调数据处理，mg表示预训练。</td>
    </tr>
    <tr>
      <td>split</td>
      <td>str</td>
      <td>100,0,0</td>
      <td>训练集、验证集和测试集的划分比例，使用逗号分隔。</td>
    </tr>
    <tr>
      <td>create_attention_mask_in_dataloader</td>
      <td>Optional[bool]</td>
      <td>False</td>
      <td>是否在数据加载器中创建注意力掩码。</td>
    </tr>
    <tr>
      <td>no_shared_storage</td>
      <td>Optional[bool]</td>
      <td>False</td>
      <td>是否设置共享存储。</td>
    </tr>
    <tr>
      <td>dataloader_type</td>
      <td>Literal["single"]</td>
      <td>single</td>
      <td>数据加载器类型，<code>single</code>表示顺序读取。</td>
    </tr>
  </tbody>
</table>

## 并行参数（ParallelArguments）

包含分布式并行策略与显存优化相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>tp_size</td>
      <td>int</td>
      <td>1</td>
      <td>张量并行（Tensor Parallel）大小，将模型张量按列/行拆分到多张卡。</td>
    </tr>
    <tr>
      <td>fsdp_size</td>
      <td>int</td>
      <td>1</td>
      <td>全分片数据并行（FSDP）大小，将模型参数分片存储到多张卡。</td>
    </tr>
    <tr>
      <td>recompute</td>
      <td>bool</td>
      <td>False</td>
      <td>是否启用重计算，启用后通过牺牲部分性能，节省显存占用。</td>
    </tr>
    <tr>
      <td>ep_size</td>
      <td>int</td>
      <td>1</td>
      <td>专家并行（Expert Parallel）大小，适用于MoE（混合专家）模型，将不同专家拆分到多张卡。</td>
    </tr>
    <tr>
      <td>ep_fsdp_size</td>
      <td>int</td>
      <td>1</td>
      <td>专家并行组内的FSDP大小，在专家并行的基础上，对单个专家的参数进行分片。</td>
    </tr>
    <tr>
      <td>cp_size</td>
      <td>int</td>
      <td>1</td>
      <td>长序列并行（Context Parallel）大小，将输入序列的上下文拆分到多张卡。</td>
    </tr>
    <tr>
      <td>cp_type</td>
      <td>Literal[&quot;ulysses&quot;]</td>
      <td>ulysses</td>
      <td>长序列并行算法类型，目前仅支持ulysses算法。</td>
    </tr>
    <tr>
      <td>fsdp_modules</td>
      <td>List[str]</td>
      <td>[&quot;model.layers.{}&quot;, &quot;model.embed_tokens&quot;, &quot;lm_head&quot;]</td>
      <td>启用FSDP的模型层结构，必填项，不可为空列表。</td>
    </tr>
    <tr>
      <td>ignored_modules</td>
      <td>List[str]</td>
      <td>None</td>
      <td>不启用FSDP的模块列表。</td>
    </tr>
    <tr>
      <td>reshard_after_forward</td>
      <td>bool</td>
      <td>True</td>
      <td>前向传播后是否重新分片FSDP主模块的参数。</td>
    </tr>
    <tr>
      <td>shard_placement_fn</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>FSDP主模块的自定义分片放置函数，用于自定义参数分片的分配逻辑。</td>
    </tr>
    <tr>
      <td>tp_colwise</td>
      <td>List[str]</td>
      <td>[&quot;*.q_proj&quot;, &quot;*.k_proj&quot;, &quot;*.v_proj&quot;, &quot;*.gate_proj&quot;, &quot;*.up_proj&quot;]</td>
      <td>采用列拆分的张量并行模型层结构。</td>
    </tr>
    <tr>
      <td>tp_rowwise</td>
      <td>List[str]</td>
      <td>[&quot;*.o_proj&quot;, &quot;*.down_proj&quot;]</td>
      <td>采用行拆分的张量并行模型层结构。</td>
    </tr>
    <tr>
      <td>ep_modules</td>
      <td>List[str]</td>
      <td>[&quot;model.layers.{*}.mlp.experts&quot;]</td>
      <td>启用专家并行的模型层结构，仅适用于MoE模型。</td>
    </tr>
    <tr>
      <td>ep_fsdp_modules</td>
      <td>List[str]</td>
      <td>[&quot;model.layers.{*}.mlp.experts&quot;]</td>
      <td>专家并行组内启用FSDP的模型层结构。</td>
    </tr>
    <tr>
      <td>ep_dispatcher</td>
      <td>Literal[&quot;eager&quot;, &quot;fused&quot;, &quot;mc2&quot;]</td>
      <td>eager</td>
      <td>MoE专家并行的调度策略。</td>
    </tr>
    <tr>
      <td>recompute_modules</td>
      <td>List[str]</td>
      <td>[&quot;model.layers.{*}&quot;]</td>
      <td>启用激活重计算的模型层结构，与<code>recompute=True</code>配合使用。</td>
    </tr>
    <tr>
      <td>param_dtype</td>
      <td>Literal[&quot;bf16&quot;, &quot;fp16&quot;, &quot;fp32&quot;]</td>
      <td>bf16</td>
      <td>FSDP参数存储的数据类型。</td>
    </tr>
    <tr>
      <td>reduce_dtype</td>
      <td>Literal[&quot;bf16&quot;, &quot;fp16&quot;, &quot;fp32&quot;]</td>
      <td>fp32</td>
      <td>FSDP梯度归约的数据类型，默认fp32，确保梯度归约的数值稳定性，避免梯度消失/爆炸。</td>
    </tr>
    <tr>
      <td>num_to_forward_prefetch</td>
      <td>int</td>
      <td>1</td>
      <td>FSDP前向传播时预取的模块数，用于优化流水线效率。</td>
    </tr>
    <tr>
      <td>num_to_backward_prefetch</td>
      <td>int</td>
      <td>1</td>
      <td>FSDP反向传播时预取的模块数，用于优化流水线效率。</td>
    </tr>
  </tbody>
</table>

## 训练参数（TrainingArguments）

包含训练超参、优化器、模型保存与日志相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>output_dir</td>
      <td>str</td>
      <td>无（必填）</td>
      <td>训练结果输出目录，用于保存模型权重、日志、预测结果等，必填项，未指定会抛出异常。</td>
    </tr>
    <tr>
      <td>optimizer</td>
      <td>Literal[&quot;adamw&quot;]</td>
      <td>adamw</td>
      <td>优化器，目前仅支持AdamW优化器。</td>
    </tr>
    <tr>
      <td>lr</td>
      <td>float</td>
      <td>1e-5</td>
      <td>初始学习率。</td>
    </tr>
    <tr>
      <td>weight_decay</td>
      <td>float</td>
      <td>0.01</td>
      <td>权重衰减系数。</td>
    </tr>
    <tr>
      <td>adam_beta1</td>
      <td>float</td>
      <td>0.9</td>
      <td>AdamW优化器的beta1参数，控制一阶动量的指数衰减率。</td>
    </tr>
    <tr>
      <td>adam_beta2</td>
      <td>float</td>
      <td>0.95</td>
      <td>AdamW优化器的beta2参数，控制二阶动量的指数衰减率。</td>
    </tr>
    <tr>
      <td>adam_epsilon</td>
      <td>float</td>
      <td>1e-8</td>
      <td>AdamW优化器的epsilon参数。</td>
    </tr>
    <tr>
      <td>max_grad_norm</td>
      <td>float</td>
      <td>1.0</td>
      <td>梯度裁剪的最大范数，防止梯度爆炸。</td>
    </tr>
    <tr>
      <td>lr_scheduler_type</td>
      <td>Literal[&quot;cosine&quot;, &quot;linear&quot;, &quot;constant&quot;]</td>
      <td>cosine</td>
      <td>学习率调度器类型：余弦退火/线性衰减/恒定学习率。</td>
    </tr>
    <tr>
      <td>warmup_ratio</td>
      <td>float</td>
      <td>0.03</td>
      <td>线性预热占总训练步数的比例。</td>
    </tr>
    <tr>
      <td>min_lr</td>
      <td>float</td>
      <td>1e-6</td>
      <td>最小学习率，此参数仅在cosine调度器下生效。</td>
    </tr>
    <tr>
      <td>num_train_epochs</td>
      <td>float</td>
      <td>3.0</td>
      <td>总训练轮数，若<code>max_steps&gt;0</code>，该参数会被覆盖。</td>
    </tr>
    <tr>
      <td>max_steps</td>
      <td>int</td>
      <td>-1</td>
      <td>总训练步数，&gt;0时覆盖<code>num_train_epochs</code>。</td>
    </tr>
    <tr>
      <td>gradient_accumulation_steps</td>
      <td>int</td>
      <td>1</td>
      <td>梯度累积步数，将多个批次的梯度累积后再执行反向传播/参数更新。</td>
    </tr>
    <tr>
      <td>disable_shuffling</td>
      <td>bool</td>
      <td>False</td>
      <td>是否禁用训练集洗牌。</td>
    </tr>
    <tr>
      <td>seed</td>
      <td>int</td>
      <td>42</td>
      <td>训练开始时设置的随机种子。</td>
    </tr>
    <tr>
      <td>save_steps</td>
      <td>int</td>
      <td>500</td>
      <td>每X步保存一次模型权重。</td>
    </tr>
    <tr>
      <td>save_total_limit</td>
      <td>Optional[int]</td>
      <td>3</td>
      <td>权重总数限制，超过该数量时删除最早的权重。</td>
    </tr>
    <tr>
      <td>resume_from_checkpoint</td>
      <td>Optional[str]</td>
      <td>None</td>
      <td>恢复训练的权重路径。</td>
    </tr>
    <tr>
      <td>logging_steps</td>
      <td>int</td>
      <td>1</td>
      <td>每X步记录一次训练日志。</td>
    </tr>
    <tr>
      <td>stage</td>
      <td>Literal[&quot;pt&quot;, &quot;sft&quot;]</td>
      <td>sft</td>
      <td>训练阶段：预训练/指令微调。</td>
    </tr>
    <tr>
      <td>calculate_per_token_loss</td>
      <td>bool</td>
      <td>False</td>
      <td>计算交叉熵损失时，是否根据全局批次中的非填充token数量进行缩放。</td>
    </tr>
    <tr>
      <td>chunk_loss_size</td>
      <td>int</td>
      <td>None</td>
      <td>每次计算loss时使用的序列长度，分段进行loss计算可以节省显存。</td>
    </tr>
    <tr>
      <td>dataloader_num_workers</td>
      <td>int</td>
      <td>0</td>
      <td>数据加载子进程数，0表示主进程加载数据。</td>
    </tr>
    <tr>
      <td>dataloader_prefetch_factor</td>
      <td>Optional[int]</td>
      <td>None</td>
      <td>每个数据加载子进程预加载的批次数。</td>
    </tr>
    <tr>
      <td>dataloader_pin_memory</td>
      <td>bool</td>
      <td>True</td>
      <td>是否为数据加载器启用内存锁定。</td>
    </tr>
    <tr>
      <td>dataloader_persistent_workers</td>
      <td>bool</td>
      <td>False</td>
      <td>是否保持数据加载子进程存活。</td>
    </tr>
    <tr>
      <td>dataloader_drop_last</td>
      <td>bool</td>
      <td>False</td>
      <td>当数据集大小不能被batch size整除时，是否丢弃最后一个不完整的批次。</td>
    </tr>
    <tr>
      <td>per_device_train_batch_size</td>
      <td>int</td>
      <td>8</td>
      <td>每张卡的训练batch size大小。</td>
    </tr>
    <tr>
      <td>gdn_chunk_size</td>
      <td>int</td>
      <td>64</td>
      <td>Gated DeltaNet网络计算时矩阵的分块的数量。</td>
    </tr>
    <tr>
      <td>save_only_model</td>
      <td>bool</td>
      <td>False</td>
      <td>保存权重时，是否只保存模型参数，而不保存优化器、调度器及随机数状态。</td>
    </tr>
    <tr>
      <td>save_async</td>
      <td>bool</td>
      <td>False</td>
      <td>是否异步保存权重。</td>
    </tr>
    <tr>
      <td>save_epochs</td>
      <td>int</td>
      <td>1</td>
      <td>保存权重的间隔轮数。</td>
    </tr>
    <tr>
      <td>save_hf_weights</td>
      <td>bool</td>
      <td>True</td>
      <td>训练完成后，是否保存HuggingFace格式的权重。</td>
    </tr>
  </tbody>
</table>

## 融合算子

包含融合算子使能相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>use_fused_rmsnorm</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使能融合rmsnorm算子。</td>
    </tr>
    <tr>
      <td>moe_grouped_gemm</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使能融合gmm算子。</td>
    </tr>
    <tr>
      <td>use_fused_rotary_pos_emb</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使能融合RoPE算子。</td>
    </tr>
    <tr>
      <td>use_flash_attn</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使能FA算子。</td>
    </tr>
    <tr>
      <td>use_triton_gdn</td>
      <td>bool</td>
      <td>False</td>
      <td>是否使能triton融合算子加速Gated DeltaNet网络计算。</td>
    </tr>
  </tbody>
</table>

## profiling参数

包含性能分析数据采集相关参数。

<table>
  <thead>
    <tr>
      <th>参数名</th>
      <th>类型</th>
      <th>默认值</th>
      <th>详细说明</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>profile</td>
      <td>bool</td>
      <td>False</td>
      <td>是否开启profiling采集数据。</td>
    </tr>
    <tr>
      <td>profile_step_start</td>
      <td>int</td>
      <td>0</td>
      <td>开启采集数据的步骤（包含该步），需大于等于0。</td>
    </tr>
    <tr>
      <td>profile_step_end</td>
      <td>int</td>
      <td>-1</td>
      <td>结束采集数据的步骤（不包含该步），-1表示采集数据至训练结束。</td>
    </tr>
    <tr>
      <td>profile_ranks</td>
      <td>List[int]</td>
      <td>[-1]</td>
      <td>采集数据的卡号，-1表示采集所有rank的profiling数据。</td>
    </tr>
    <tr>
      <td>profile_level</td>
      <td>str</td>
      <td>level0</td>
      <td>数据采集水平：level_none/level0/level1/level2，级别越高采集信息越多。</td>
    </tr>
    <tr>
      <td>profile_export_type</td>
      <td>str</td>
      <td>text</td>
      <td>profiling结果导出类型：文本格式/数据库格式。</td>
    </tr>
    <tr>
      <td>profile_data_simplification</td>
      <td>bool</td>
      <td>False</td>
      <td>是否启用profiling的数据简化模式。</td>
    </tr>
    <tr>
      <td>profile_with_cpu</td>
      <td>bool</td>
      <td>False</td>
      <td>是否在profiling中采集CPU数据。</td>
    </tr>
    <tr>
      <td>profile_with_stack</td>
      <td>bool</td>
      <td>False</td>
      <td>是否在profiling中采集调用栈信息。</td>
    </tr>
    <tr>
      <td>profile_with_memory</td>
      <td>bool</td>
      <td>False</td>
      <td>是否在profiling中采集内存分配和使用情况。</td>
    </tr>
    <tr>
      <td>profile_record_shapes</td>
      <td>bool</td>
      <td>False</td>
      <td>是否在profiling中采集张量形状信息。</td>
    </tr>
    <tr>
      <td>profile_save_path</td>
      <td>str</td>
      <td>./profile</td>
      <td>profiling数据采集保存路径。</td>
    </tr>
  </tbody>
</table>


# megatron参数对照

<table>
  <thead>
    <tr>
      <th>参数类型</th>
      <th>FSDP2参数</th>
      <th>megatron参数</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td rowspan="2">ModelArguments</td>
      <td>model_name_or_path</td>
      <td>--load</td>
    </tr>
    <tr>
      <td>tokenizer_name_or_path</td>
      <td>--tokenizer-name-or-path</td>
    </tr>
    <tr>
      <td rowspan="4">DataArguments</td>
      <td>dataset</td>
      <td>--data-path</td>
    </tr>
    <tr>
      <td>cutoff_len</td>
      <td>--seq-length</td>
    </tr>
    <tr>
      <td>preprocessing_num_workers</td>
      <td>--workers</td>
    </tr>
    <tr>
      <td>packing</td>
      <td>--pack</td>
    </tr>
    <tr>
      <td rowspan="5">ParallelArguments</td>
      <td>tp_size</td>
      <td>--tensor-model-parallel-size</td>
    </tr>
    <tr>
      <td>ep_size</td>
      <td>--expert-model-parallel-size</td>
    </tr>
    <tr>
      <td>cp_size</td>
      <td>--context-parallel-size</td>
    </tr>
    <tr>
      <td>cp_type</td>
      <td>--context-parallel-algo</td>
    </tr>
    <tr>
      <td>recompute</td>
      <td>--recompute-granularity</td>
    </tr>
    <tr>
      <td rowspan="9">TrainingArguments</td>
      <td>per_device_train_batch_size</td>
      <td>--micro-batch-size</td>
    </tr>
    <tr>
      <td>disable_shuffling</td>
      <td>--no-shuffle</td>
    </tr>
    <tr>
      <td>lr_scheduler_type</td>
      <td>--lr-decay-style</td>
    </tr>
    <tr>
      <td>warmup_ratio</td>
      <td>--lr-warmup-fraction</td>
    </tr>
    <tr>
      <td>recompute</td>
      <td>--recompute-method</td>
    </tr>
    <tr>
      <td>max_steps</td>
      <td>--train-iters</td>
    </tr>
    <tr>
      <td>save_steps</td>
      <td>--save-interval</td>
    </tr>
    <tr>
      <td>logging_steps</td>
      <td>--log-interval</td>
    </tr>
    <tr>
      <td>卡数*per_device_train_batch_size*gradient_accumulation_steps</td>
      <td>--global-batch-size</td>
    </tr>
  </tbody>
</table>