# CCLoRA

## 问题分析

LoRA微调算法通过更新附加在冻结预训练模型权重上的低秩矩阵，实现对模型的高效微调。过程中串行执行冻结预训练权重及低秩权重的前向和反向，在分布式场景，会造成冗余通信耗时。实际预训练分支与更新分支没有依赖关系，可通过异步方式实现通算优化；此外，通过挖掘数学等价的计算方式，可覆盖各种场景，实现极致性能优化。

## 解决方案

1. 将单一流水优化为通信计算双流水线
![输入图片说明](https://foruda.gitee.com/images/1722944695028060121/d24c8bcf_8362322.png "CCLoRA.png")

2. 数学等价方式，合并通信，从而利用MC2进一步加速

   SP&Row场景，B是线性层，各卡参数一致，通过变换后仅需进行一次allgather(dy)
   
3. 数学等价方式，优化scale逻辑
  
  $$
  x\ shape:[B, S, H] \\ 
  Y = Wx + \lambda BAx \\
  = Wx + B(\lambda A)x   \ \ \ \  (B * S < H)\ \  \\
  = (W + B(\lambda A)x   \ \ \ \  (B * S >= H)   \ \ 
  $$
  

## 使用方法

RC2以上版本，LoRA微调场景，算法与PP、VPP、分布式优化器等场景兼容

通过设置--lora-fusion开启CCLoRA的加速，在开启了TP或SP场景，加入--use-fused-mlp启用<td><a href="fused_mlp.md">Fused_MLP</a></td>算法，针对LoRA的MLP模块进一步进行计算通信掩盖提高性能。

## 使用效果

|       模型        | NPU  |  TP  |  PP  | SP   | 基线吞吐 | CCLoRA+吞吐 | 性能提升 |
| :---------------: | :--: | :--: | :--: | ---- | :------: | :---------: | :------: |
|  Llama2-7B-动态   |  8   |  8   |  1   | √    |   9.72   |    13.64    |  40.3%   |
|  Llama2-7B-pack   |  8   |  8   |  1   | √    |   5.97   |    6.65     |  11.5%   |
| Mixtral-7*8B-动态 |  8   |  1   |  4   | ×    |  13.97   |    20.55    |  47.1%   |
|  Llama2-70B-动态  |  8   |  1   |  4   | ×    |  13.50   |    14.75    |   9.3%   |